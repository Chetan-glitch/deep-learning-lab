{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417dbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779a4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\cheta\\\\Downloads\\\\poems-100.csv\")\n",
    "\n",
    "text = \" \".join(df[\"text\"].astype(str)).lower()\n",
    "tokens = text.split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d7941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 6989\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(tokens))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1974ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: torch.Size([24729, 5])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "for i in range(len(tokens) - seq_len):\n",
    "    input_sequences.append(tokens[i:i+seq_len])\n",
    "    target_words.append(tokens[i+seq_len])\n",
    "\n",
    "X = torch.tensor([[word2idx[w] for w in seq] for seq in input_sequences])\n",
    "y = torch.tensor([word2idx[w] for w in target_words])\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Dataset Shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float()\n",
    "        x = self.input_proj(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.output_layer(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c71536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.output_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e21617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.005):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output[:, -1, :], batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aca6eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training OneHot Model\n",
      "\n",
      "Epoch 1: Loss = 7.5875\n",
      "Epoch 2: Loss = 7.1129\n",
      "Epoch 3: Loss = 6.6470\n",
      "Epoch 4: Loss = 5.9717\n",
      "Epoch 5: Loss = 5.0049\n",
      "Epoch 6: Loss = 3.9178\n",
      "Epoch 7: Loss = 3.0325\n",
      "Epoch 8: Loss = 2.3740\n",
      "Epoch 9: Loss = 1.9306\n",
      "Epoch 10: Loss = 1.6362\n",
      "\n",
      "Training Embedding Model\n",
      "\n",
      "Epoch 1: Loss = 7.5376\n",
      "Epoch 2: Loss = 6.6448\n",
      "Epoch 3: Loss = 5.5398\n",
      "Epoch 4: Loss = 4.3279\n",
      "Epoch 5: Loss = 3.3728\n",
      "Epoch 6: Loss = 2.6745\n",
      "Epoch 7: Loss = 2.1364\n",
      "Epoch 8: Loss = 1.7716\n",
      "Epoch 9: Loss = 1.5158\n",
      "Epoch 10: Loss = 1.3288\n",
      "Epoch 11: Loss = 1.1967\n",
      "Epoch 12: Loss = 1.1393\n",
      "Epoch 13: Loss = 1.0832\n",
      "Epoch 14: Loss = 1.0408\n",
      "Epoch 15: Loss = 1.0119\n",
      "Epoch 16: Loss = 0.9737\n",
      "Epoch 17: Loss = 0.9668\n",
      "Epoch 18: Loss = 0.9503\n",
      "Epoch 19: Loss = 0.9423\n",
      "Epoch 20: Loss = 0.9504\n"
     ]
    }
   ],
   "source": [
    "model_onehot = OneHotRNN(vocab_size, 128)\n",
    "model_embed = EmbeddingRNN(vocab_size, 128, 128)\n",
    "\n",
    "print(\"\\nTraining OneHot Model\\n\")\n",
    "loss_onehot = train_model(model_onehot, epochs=10)\n",
    "\n",
    "print(\"\\nTraining Embedding Model\\n\")\n",
    "loss_embed = train_model(model_embed, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491f7384",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(loss_onehot, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOneHot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss_embed, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_onehot, marker='o', label=\"OneHot\")\n",
    "plt.plot(loss_embed, marker='s', label=\"Embedding\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedac09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(\u001b[43mx\u001b[49m, num_classes\u001b[38;5;241m=\u001b[39mvocab_size)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "X = torch.nn.functional.one_hot(x, num_classes=vocab_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd92eb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X exists? True\n"
     ]
    }
   ],
   "source": [
    "print(\"X exists?\", 'X' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a042182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 5000\n",
    "vocab = vocab[:max_vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea30eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9dbd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onehot = OneHotRNN(vocab_size, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd0daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6989\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18468a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([24729, 5])\n",
      "Target Shape: torch.Size([24729])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(tokens) - seq_len):\n",
    "    inputs.append(tokens[i:i+seq_len])\n",
    "    targets.append(tokens[i+seq_len])\n",
    "\n",
    "X = torch.tensor([[word2idx[w] for w in seq] for seq in inputs])\n",
    "y = torch.tensor([word2idx[w] for w in targets])\n",
    "\n",
    "print(\"Input Shape:\", X.shape)\n",
    "print(\"Target Shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b633d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250fff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704b09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextRNN(vocab_size, 128, 128)\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108ee47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 7.7386\n",
      "Epoch 2, Loss = 6.7589\n",
      "Epoch 3, Loss = 5.5616\n",
      "Epoch 4, Loss = 4.5317\n",
      "Epoch 5, Loss = 3.8472\n",
      "Epoch 6, Loss = 3.3816\n",
      "Epoch 7, Loss = 3.0547\n",
      "Epoch 8, Loss = 2.8040\n",
      "Epoch 9, Loss = 2.7050\n",
      "Epoch 10, Loss = 2.5688\n",
      "Epoch 11, Loss = 2.5035\n",
      "Epoch 12, Loss = 2.4212\n",
      "Epoch 13, Loss = 2.4076\n",
      "Epoch 14, Loss = 2.3302\n",
      "Epoch 15, Loss = 2.3402\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output[:, -1, :], batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65b6ab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m         input_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[idx]])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlove\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def generate(model, start_word, length=20):\n",
    "    model.eval()\n",
    "    words = [start_word]\n",
    "\n",
    "    input_seq = torch.tensor([[word2idx[start_word]]])\n",
    "\n",
    "    for _ in range(length):\n",
    "        output = model(input_seq)\n",
    "        prob = torch.softmax(output[0, -1], dim=0)\n",
    "        idx = torch.multinomial(prob, 1).item()\n",
    "\n",
    "        words.append(idx2word[idx])\n",
    "        input_seq = torch.tensor([[idx]])\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "print(generate(model, \"love\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
